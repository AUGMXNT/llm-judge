# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/shisa/ft-ablation/shisa-bad-7b-v1 --model-id shisa-bad-7b-v1 --num-gpus-total 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/shisa/ft-ablation/shisa-gamma-7b-v1 --model-id shisa-gamma-7b-v1 --num-gpus-total 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/shisa/zero-extra/shisa-mega-dpo-7b-v1.1 --model-id shisa-mega-dpo-7b-v1.1 --num-gpus-total 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/shisa/zero-extra/shisa-mega-7b-v1.2 --model-id shisa-mega-7b-v1.2 --num-gpus-total 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/shisa/augmxnt_shisa-mega-7b-v1.2-dpo --model-id shisa-mega-7b-v1.2-dpo --num-gpus-total 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /workspace/llm-jp-13b-instruct-full-jaster-v1.0 --model-id llm-jp-13b-instruct-full-jaster-v1.0 
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /workspace/llm-jp_llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 --model-id llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/hf/moneyforward_houou-instruction-7b-v1 --model-id houou-instruction-7b-v1 --num-gpus-total 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/hf/moneyforward_houou-instruction-7b-v1 --model-id houou-instruction-7b-v1-correctedtemplate --num-gpus-total 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/hf/moneyforward_houou-instruction-7b-v1 --model-id houou-instruction-7b-v1-correctedtemplate2 --num-gpus-total 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/hf/tokyotech-llm_Swallow-7b-instruct-hf --model-id tokyotech-llm_Swallow-7b-instruct-hf --num-gpus-total 2 --num-gpus-per-model 1
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/shisa-7b-v1 --model-id shisa-7b-v1-fullprompt --num-gpus-total 2 --num-gpus-per-model 1
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path tokyotech-llm/Swallow-13b-instruct-hf --model-id tokyotech-llm_Swallow-13b-instruct-hf --num-gpus-total 2 --num-gpus-per-model 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path TheBloke/Swallow-70B-instruct-AWQ --model-id Swallow-70b-instruct-AWQ --num-gpus-total 2 --num-gpus-per-model 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path TheBloke/Swallow-70B-instruct-GPTQ --model-id Swallow-70b-instruct-GPTQ --num-gpus-total 2 --num-gpus-per-model 2
# time CUDA_VISIBLE_DEVICES=0 python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path TheBloke/shisa-7B-v1-GPTQ --model-id shisa-7B-v1-GPTQ --num-gpus-total 2 --num-gpus-per-model 1
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path TheBloke/shisa-7B-v1-AWQ --model-id shisa-7B-v1-AWQ --num-gpus-total 2 --num-gpus-per-model 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/gptq/TheBloke_Swallow-70B-instruct-GPTQ --model-id Swallow-70b-instruct-GPTQ --num-gpus-total 2 --num-gpus-per-model 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /models/llm/gptq/TheBloke_Xwin-LM-70B-V0.1-GPTQ --model-id Xwin-LM-70B-V0.1-GPTQ --num-gpus-total 2 --num-gpus-per-model 2
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path Qwen/Qwen-72b-Chat --model-id Qwen-72b-Chat --num-gpus-total 4 --num-gpus-per-model 4
# time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /workspace/rinna_nekomata-14b-instruction --model-id nekomata-14b-instruction-correctedprompt --num-gpus-total 1 --num-gpus-per-model 1
time python gen_model_answer.py --bench-name japanese_mt_bench --num-choices 4 --max-turns 1 --model-path /workspace/rinna_nekomata-14b-instruction --model-id nekomata-14b-instruction-correctedprompt-hf --num-gpus-total 1 --num-gpus-per-model 1
